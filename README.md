# sea_policy_intelligence
This repository showcases "SEA Policy Intelligence," a personal project utilizing an AWS-powered data pipeline to extract and analyze information on government projects, initiatives, and their impacts across Southeast Asian countries, sourced from the vast Common Crawl web archive, all for the purpose of generating public policy insights.

## Project Milestones

### Week 1: Project Setup & Planning
- Define project scope and objectives.
- Research Common Crawl and AWS data pipeline tools.
- Set up repository structure and initial documentation.
- Set up AWS account and configure credentials.

### Week 2: Data Acquisition
- Explore Common Crawl datasets.
- Write scripts to extract relevant data from Common Crawl.
- Store raw data in AWS S3.

### Week 3: Data Processing Pipeline
- Design ETL (Extract, Transform, Load) pipeline architecture.
- Implement data cleaning and preprocessing scripts.
- Automate pipeline using AWS services (e.g., Lambda, Glue).

### Week 4: Data Analysis & Storage
- Develop methods to analyze government projects and initiatives.
- Store processed data in a queryable format (e.g., AWS RDS, DynamoDB, or S3 with Athena).

### Week 5: Insights Generation
- Implement algorithms to generate public policy insights.
- Visualize findings using charts or dashboards (e.g., AWS QuickSight, Jupyter Notebooks).

### Week 6: Testing & Documentation
- Write unit and integration tests for pipeline components.
- Document code and usage instructions.
- Review and refine data analysis and insights.

### Week 7: Deployment & Final Review
- Set up automated deployment (CI/CD) if needed.
- Optimize AWS resources for cost and performance.
- Final project review and prepare a demo or presentation.

---